{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b16b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([1, 1, 256, 256])\n",
      "Output shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------- Involution (group = input channels) ----------\n",
    "# --- changes only: add 'groups' to Involution/InvHead/MobileNetV3_INV ---\n",
    "\n",
    "class Involution(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=7, stride=1, reduction=4,\n",
    "                 kernel_norm: str = \"l2\", softmax_temp: float = 1.0,\n",
    "                 groups: int | None = None):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.reduction = max(1, reduction)\n",
    "        self.kernel_norm = kernel_norm\n",
    "        self.softmax_temp = softmax_temp\n",
    "\n",
    "        # groups: number of channel groups for dynamic kernels\n",
    "        self.groups = channels if (groups is None) else int(groups)\n",
    "        assert self.groups >= 1 and channels % self.groups == 0, \\\n",
    "            f\"'groups' must divide channels: got C={channels}, groups={self.groups}\"\n",
    "\n",
    "        hidden = max(1, channels // self.reduction)\n",
    "\n",
    "        # C -> hidden -> (k^2 * groups)\n",
    "        self.reduce = nn.Conv2d(channels, hidden, kernel_size=1, bias=False)\n",
    "        self.bn     = nn.BatchNorm2d(hidden)\n",
    "        self.act    = nn.ReLU(inplace=True)\n",
    "        self.kproj  = nn.Conv2d(hidden, (kernel_size * kernel_size) * self.groups,\n",
    "                                kernel_size=1, bias=True)\n",
    "\n",
    "        self.pool_for_k = nn.AvgPool2d(stride, stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "    def _normalize_kernel(self, ker: torch.Tensor) -> torch.Tensor:\n",
    "        if self.kernel_norm == \"softmax\":\n",
    "            B, G, k, _, H, W = ker.shape\n",
    "            ker = F.softmax(ker.view(B, G, k*k, H, W) / self.softmax_temp, dim=2)\n",
    "            return ker.view(B, G, k, k, H, W)\n",
    "        if self.kernel_norm == \"l2\":\n",
    "            ker = ker - ker.mean(dim=(2, 3), keepdim=True)\n",
    "            denom = ker.norm(dim=(2, 3), keepdim=True).clamp_min(1e-6)\n",
    "            ker = ker / denom\n",
    "        return ker\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_kernels(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        k = self.kernel_size\n",
    "        xk = self.pool_for_k(x)\n",
    "        K  = self.kproj(self.act(self.bn(self.reduce(xk))))            # [B, k^2*G, H', W']\n",
    "        if K.shape[-2:] != (H, W):\n",
    "            K = F.interpolate(K, size=(H, W), mode=\"bilinear\", align_corners=True)\n",
    "        K = K.view(B, self.groups, k, k, H, W)                         # [B,G,k,k,H,W]\n",
    "        return self._normalize_kernel(K)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        k, G = self.kernel_size, self.groups\n",
    "        assert C % G == 0\n",
    "        groupC = C // G\n",
    "\n",
    "        K = self.get_kernels(x)                                        # [B,G,k,k,H,W]\n",
    "        x_unfold = F.unfold(x, kernel_size=k, padding=k//2)            # [B,C*k*k,H*W]\n",
    "        x_unfold = x_unfold.view(B, C, k, k, H, W).view(B, G, groupC, k, k, H, W)\n",
    "        out = (x_unfold * K.unsqueeze(2)).sum(dim=(3,4)).view(B, C, H, W)\n",
    "        return out\n",
    "\n",
    "\n",
    "class InvHead(nn.Module):\n",
    "    def __init__(self, channels, reduce=4, k=9, inv_reduction=4,\n",
    "                 kernel_norm=\"l2\", softmax_temp=1.0, inv_groups: int | None = None):\n",
    "        super().__init__()\n",
    "        hidden = max(8, channels // reduce)\n",
    "        self.hidden = hidden\n",
    "\n",
    "        self.reduce = nn.Conv2d(channels, hidden, 1, bias=False)\n",
    "        self.bn1    = nn.BatchNorm2d(hidden)\n",
    "        self.act    = nn.ReLU(inplace=True)\n",
    "\n",
    "        # 'inv_groups' applies over HIDDEN channels\n",
    "        if inv_groups is None:\n",
    "            inv_groups = hidden  # depthwise by default\n",
    "        else:\n",
    "            inv_groups = int(inv_groups)\n",
    "        assert hidden % inv_groups == 0, \\\n",
    "            f\"'inv_groups' must divide hidden={hidden}; got {inv_groups}\"\n",
    "\n",
    "        self.inv = Involution(\n",
    "            channels=hidden, kernel_size=k, stride=1,\n",
    "            reduction=inv_reduction, kernel_norm=kernel_norm,\n",
    "            softmax_temp=softmax_temp, groups=inv_groups\n",
    "        )\n",
    "\n",
    "        self.expand = nn.Conv2d(hidden, channels, 1, bias=False)\n",
    "        self.bn2    = nn.BatchNorm2d(channels)\n",
    "        self.gamma  = nn.Parameter(torch.tensor(0.05))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.act(self.bn1(self.reduce(x)))\n",
    "        y = self.inv(y)\n",
    "        y = self.bn2(self.expand(y))\n",
    "        return self.act(x + self.gamma * y)\n",
    "    \n",
    "# ------------------------------------------------------------------\n",
    "# SIMAM: Simple, Parameter-Free Attention (SimAM)\n",
    "# ------------------------------------------------------------------\n",
    "class SIMAM(nn.Module):\n",
    "    \"\"\"\n",
    "    SimAM: A Simple, Parameter-free Attention Module\n",
    "    Applied element-wise using an energy function.\n",
    "    Paper: https://arxiv.org/abs/2103.06215\n",
    "    \"\"\"\n",
    "    def __init__(self, e_lambda: float = 1e-4):\n",
    "        super().__init__()\n",
    "        self.e_lambda = e_lambda\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, C, H, W]\n",
    "        \"\"\"\n",
    "        b, c, h, w = x.size()\n",
    "        n = h * w - 1\n",
    "\n",
    "        # Center w.r.t channel-wise spatial mean\n",
    "        mu = x.mean(dim=[2, 3], keepdim=True)\n",
    "        x_centered = x - mu\n",
    "\n",
    "        var = (x_centered ** 2).sum(dim=[2, 3], keepdim=True) / max(n, 1)\n",
    "        e = x_centered ** 2 / (4 * (var + self.e_lambda)) + 0.5\n",
    "\n",
    "        attn = torch.sigmoid(e)\n",
    "        return x * attn\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Basic UNet building blocks\n",
    "# ------------------------------------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv -> BN -> ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    Upscaling then double conv.\n",
    "    Uses bilinear upsampling by default.\n",
    "    SIMAM is applied to the skip connection before concatenation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,     # channels coming from previous (lower) layer\n",
    "        skip_channels: int,   # channels from encoder skip\n",
    "        out_channels: int,\n",
    "        bilinear: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        else:\n",
    "            # If you prefer transposed conv, uncomment and adjust channels\n",
    "            # self.up = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2)\n",
    "            # For simplicity, keep bilinear for now:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        # After upsample: concat([skip, up(x)]) -> DoubleConv\n",
    "        self.conv = DoubleConv(in_channels + skip_channels, out_channels)\n",
    "        self.attn = SIMAM()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
    "        # x: from decoder lower level\n",
    "        # skip: from encoder (same spatial level)\n",
    "        x = self.up(x)\n",
    "\n",
    "        # Handle size mismatch if input is not divisible by 2 multiple times\n",
    "        diff_y = skip.size(2) - x.size(2)\n",
    "        diff_x = skip.size(3) - x.size(3)\n",
    "        if diff_x != 0 or diff_y != 0:\n",
    "            x = F.pad(\n",
    "                x,\n",
    "                [diff_x // 2, diff_x - diff_x // 2,\n",
    "                 diff_y // 2, diff_y - diff_y // 2]\n",
    "            )\n",
    "\n",
    "        # Apply SIMAM on skip connection\n",
    "        skip = self.attn(skip)\n",
    "\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# UNet with Involution at bottleneck + SIMAM on skip connections\n",
    "# ------------------------------------------------------------------\n",
    "# Assumes you already defined:\n",
    "# - class Involution(nn.Module): ...\n",
    "# - class InvHead(nn.Module): ...\n",
    "# from your provided code.\n",
    "\n",
    "class UNetInvSimAM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 1,\n",
    "        num_classes: int = 1,\n",
    "        base_c: int = 64,\n",
    "        bilinear: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.inc   = DoubleConv(in_channels, base_c)\n",
    "        self.down1 = Down(base_c,       base_c * 2)\n",
    "        self.down2 = Down(base_c * 2,   base_c * 4)\n",
    "        self.down3 = Down(base_c * 4,   base_c * 8)\n",
    "        self.down4 = Down(base_c * 8,   base_c * 16)\n",
    "\n",
    "        # Bottleneck: use InvHead (which internally uses Involution)\n",
    "        self.bottleneck = InvHead(\n",
    "            channels=base_c * 16,\n",
    "            reduce=4,\n",
    "            k=7,               # kernel size for involution at bottleneck\n",
    "            inv_reduction=4,\n",
    "            kernel_norm=\"l2\",\n",
    "            softmax_temp=1.0,\n",
    "            inv_groups=None,   # depthwise over hidden by default\n",
    "        )\n",
    "\n",
    "        # Decoder with SIMAM on skips\n",
    "        self.up1 = Up(\n",
    "            in_channels=base_c * 16,\n",
    "            skip_channels=base_c * 8,\n",
    "            out_channels=base_c * 8,\n",
    "            bilinear=bilinear,\n",
    "        )\n",
    "        self.up2 = Up(\n",
    "            in_channels=base_c * 8,\n",
    "            skip_channels=base_c * 4,\n",
    "            out_channels=base_c * 4,\n",
    "            bilinear=bilinear,\n",
    "        )\n",
    "        self.up3 = Up(\n",
    "            in_channels=base_c * 4,\n",
    "            skip_channels=base_c * 2,\n",
    "            out_channels=base_c * 2,\n",
    "            bilinear=bilinear,\n",
    "        )\n",
    "        self.up4 = Up(\n",
    "            in_channels=base_c * 2,\n",
    "            skip_channels=base_c,\n",
    "            out_channels=base_c,\n",
    "            bilinear=bilinear,\n",
    "        )\n",
    "\n",
    "        self.outc = nn.Conv2d(base_c, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)       # [B, base_c, H,   W  ]\n",
    "        x2 = self.down1(x1)    # [B, 2C,    H/2, W/2]\n",
    "        x3 = self.down2(x2)    # [B, 4C,    H/4, W/4]\n",
    "        x4 = self.down3(x3)    # [B, 8C,    H/8, W/8]\n",
    "        x5 = self.down4(x4)    # [B,16C,   H/16, W/16]\n",
    "\n",
    "        # Bottleneck with InvHead (Involution inside)\n",
    "        x5 = self.bottleneck(x5)\n",
    "\n",
    "        # Decoder with SIMAM skip attention\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x,  x3)\n",
    "        x = self.up3(x,  x2)\n",
    "        x = self.up4(x,  x1)\n",
    "\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Example usage\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy input\n",
    "    x = torch.randn(1, 1, 256, 256)\n",
    "\n",
    "    # Make sure your Involution + InvHead definitions are above this.\n",
    "    model = UNetInvSimAM(in_channels=1, num_classes=1, base_c=32)\n",
    "    y = model(x)\n",
    "    print(\"Input shape :\", x.shape)\n",
    "    print(\"Output shape:\", y.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
